پروژه پایان ترم داده کاوی
استاد دکتر وفایی جهان

پروژه انتخابی من پروژه 1(انتساب ایموجی متناسب به متن ) 
برای انجام پروژه ابتدا کارهای زیر صورت گرفت :
•	از بین 100 هزار مجموعه داده کامنت های دیجی کالا 2000 رکورد انتخاب شد.
•	از لینک ارسالی خانم انبایی جهت انتخاب ایموجی ، 10 تا ایموجی دلخواه نیز انتخاب شد.
•	به صورت دستی برچسب دهی صورت گرفت 
سپس پیاده سازی انجام که در ابتدا کتابخانه های مورد نیاز را فراخوانی می شود .
کتابخانه های زیر:
  : Nltk  برای متن است چون متن برای خواندن و تمیز کردن متن ها ازش استفاده شد .
Stopwords  : کلمات توقف را حذف می کند .کلمات توقف یک سری از کلمه که زیاد به کار برده میشه مثل ( آن ،به ، که و... ) خیلی زیاد به کاربرده می شود در کامنت ها و معنی خاصی هم ندارد را باید حذف کنیم و یک سری کلمه که کم تکرار می شوند و باز هم بار معنایی ندارند را باید حذف شوند که با استفاده از stopwords این کلمات را حذف و متوقف میکنیم .
کلماتی مثل ( اعدا د ، آدرس ، ایمیل که مفید نیستن برای ما و باید حذف شوند .
nltk استاپ وورد داره و چون کتمنت هاب ما فارسی هستند علاوه برا انگلیسی برای فارسی هم از hazm باید stopwords داشته باشیم که در tfidf استفاده کردیم .
hazm : چون متن فارسی هست از هضم برای پردازش و تصحیح متن استفاده و بهتره برای شبکه و متن که میخواهیم انجام بدیم از هضم استفاده کنیم .

در مرحله بعد فراخوانی کتابخانه هایی که ازشون استفاده شد. مثله numpy ، pandas ، .....
وقتی کتابخانه هایی که برای پردازش نیاز هست و فراخوانی کردیم برای نوشتن کد از توابع و کلاسهای موجود استفاده شد .
از df.shape استفاده شد جهت تبدیل شیپ برای کارهای که نیار هست تغییر زاویه بدیم .
لیبل ها و کامنت ها و دیتاها تبدیل به شیپ شد . و در مرحله بعد یه سری پیش پردازش Df1 صدا زدیم و تاریخ و خذف کردم  و فقط کامنت و لیبل کپی کردیم تو df1 که بتوانیم ازشون استفاده کنیم .
از داده ها کپی گرفته شد چون یکسری تغییرات روی داده ها انجام میشود.
Df2(category id) لیبل ها متن هست بر اساس id category به ازای هر کدام از آن ها  یک شماره در نظر گرفته شد .
 
تعداد تکرار کلمات توی کامنت ها نشان داده شده است . چند بار از Label 1 و چند بار از Label2 تکرار شده است.
در 2000 کامنت از هر ایموجی چند بار تکرار شده است.
یک سری داده  داریم که تمیز نیستن و یک ایموجی دوبار تکرار شده وزن کمتری میگیره نسبت به اونیکه 100 بار تکرار شده و با مطمئن شد که مجموعه داده ها بهتر است و متناسب بین داده هست.
در قسمت پیش پرداز داده ها چند تا کار باید انجام بشه اولین کار توکن سازی کردن به یک شکل نرمال درآوردن کامنت ها / حذف کلمات اضافی ، حذف کاما ، نقطه ها و .... تمیز و خوشگل کردن متن از طریق نرمال سازی انجام داده می شود.
•	نرمال سازی  برای حذف کلمات  اضافی 
•	توکن سازی  برای جمله داریم و برای کلمه 
در اینجا می توان از nltk ها استفاده کرد اما چون به زبان فارسی هست و برای پردازش متن فارسی از hazm استفاده میکنیم.
 
در این قسمت ابتدا توکن را صدا کردیم و بعد یک جمله را توکن سازی کردیم و سپس یک کلمه 
حالا لیستی از کلمات داریم که توکن سازی شدن و جدا جدا داریم 
حرف های اضافه حذف شده ، علائم نگارشی مشخص شده 
و کلمات توقف سازی شده و حذف کردیم و باکد tfifdf مستقیم اینکار انجام شده .
حالا میخواهیم مدل مون را بسازیم در این مرحله Vectorizer کردن است . وکتورها را به حالتی در میاریم که سیستم مجسم سازی میکند.
در این مرحله چند روش وجود داره که روشی که در اینجا استفاده شد از tfidf  استفاده شد.
Backupword لیستی از کلمات را در میاورد و بر اساس آن کلمات مدل سازی را انجام می دهد و بدی که این روش داره مثلا  در یک دسته تعداد کلمات زیاد است و در یک دسته تعداد کلمات کم مثلا  در لیبل 1 کلمه خوشحالم 100 بار تکرار شده که وزن زیاد میشه و در لیبل 10 کلمه خوشحالم 3 بار تکرار شده که وزن آن کم می باشد.

tfidf = TfidfVectorizer(lowercase=False, preprocessor=normalizer.normalize, tokenizer=tokens,ngram_range=(1, 2))

اما در  tfidf  که د راینجا استفاده شده لیستی از کلمات لما را بیرون میاره بر اساس آن به صورت شکل در میاره که سیستم را بشناسه که بهش بردارسازی گفته میشه که تعداد دفعات تکرار یک کلمه را در نظر میگیرد.
dcom=df2.comment
features = tfidf.fit_transform(dcom).toarray()

کامنتها را در dcom ریخیتم که کارش صدا ردن کامنتهااست .
در بالا tfidf وکتور کردیم برای سیستم و حال میخواهیم مدل را بسازیم براساس آن یک feature ویژگی هایی را بیرون می کشیم برای مدل سازی به یک سری ویژگی نیاز داریم که در اینجا بیرون میاریم اون ویژگی ها را 

labels = df2.category_id

print("Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)" %(features.shape))

در ادامه گفتیم 3 تا اصطلاح توی هر کامنت پیدا کنه و 3 تا کلمه ایی که مرتبط هستن را پیدا کند.   N=3
X = df2['Label'] # Collection of documents
y = df2['comment'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)

X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.20,
                                                    random_state = 0)

لیبل  = x       ،    کامنت  = y
80%  آموزش   =  Train
20% تست  =  test_size=0.20
تعداد گام ها    random_state = 0
train_test_split  :  کتبخانه ایی که همان اول فرا خوانی کردیم که برای x  و y هم آموزش و هم تست 4 خروجی را بر میگرداند.


مدل هایی که در این برنامه استفاده شده 
برای تقسیم بندی و کلاس بندی استفاده میکنیم
 که در حلقه for بر اساس مدل accuracy  ( دقت ) حساب شد و اسم مدل و ویژگی ها و لیبل هایی که داشتیم و بر اساس این دو مدل می سازد.
متن هایی که ویژگی ها برای آن ها  تعریف شد در این قسمت مدل سازی می شوند.
طبق جدول زیر بر اساس مدل های ما در اینجا 

 
از نظر دقت و از نظر انحراف معیار محاسبه و نشان داده شده است .


پراکندگی دو مدل نیز در نمودار زیر نشان داده شده که میزان دقت را براساس اعداد بالا نمایش میدهد.

 
model = LinearSVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

از مدل LinearSVC و بعد مدل را فیت کردیم و گفته شده آموزش بده بر اساس train  
Predict بر اساس تست انجام میدیم یک مدل را آموزش میده و پیش بینی میکنه که درست انجام میشه یا خیر؟
به ازای هر یک label  یک    precision   recall  f1-score  داریم.
آیا اون لیبلی که ما زدیم برای تست با اون لیبلی که پیش بینی شده درست است یا خیر؟
 
دقت و میانگین محاسبه می شود.
conf_mat = confusion_matrix(y_test, y_pred)

ماتریس پراکندگی نشان داده می شود.
برا نمایش ماتریس پراکندگی لیبل هادر نظر گرفته می شودو با هم مقایسه میکند و لیبل را پیش بینی می کند و تعداد تکرار ها را نشان میدهد.


